[
  {
    "objectID": "modules.html",
    "href": "modules.html",
    "title": "Modules",
    "section": "",
    "text": "Topic: Introduction\n\n\n\nSet up your coding environment\nWhat Is Machine Learning?\n\n\n\n\n\nCourse Introduction\nAdvanced Methods Intro\nData\n\n\n\n\n\nBeginning of semester survey"
  },
  {
    "objectID": "modules.html#reading",
    "href": "modules.html#reading",
    "title": "Modules",
    "section": "",
    "text": "Set up your coding environment\nWhat Is Machine Learning?"
  },
  {
    "objectID": "modules.html#slides",
    "href": "modules.html#slides",
    "title": "Modules",
    "section": "",
    "text": "Course Introduction\nAdvanced Methods Intro\nData"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This course website was created by Adriana Picoral and is licensed under the CC BY-NC-SA 4.0 Creative Commons License.\nYou are free to:\n\nShare — copy and redistribute the material in any medium or format.\nAdapt — remix, transform, and build upon the material.\n\nUnder the following terms:\n\nAttribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made.\nNon Commercial — You may not use the material for commercial purposes. (Remove this line if commercial use is permitted.)\nShare Alike — If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. (Include this if Share Alike applies.)"
  },
  {
    "objectID": "slides-01-01.html#hello",
    "href": "slides-01-01.html#hello",
    "title": "Introduction",
    "section": "Hello",
    "text": "Hello\n\nAdriana Picoral (PhD, you can call me Adriana)\npicorala@montclair.edu\nOffice: Schmitt 374\nOffice hours: Tuesdays and Thursdays, 1:45pm to 3:30pm"
  },
  {
    "objectID": "slides-01-01.html#active-learning",
    "href": "slides-01-01.html#active-learning",
    "title": "Introduction",
    "section": "Active Learning",
    "text": "Active Learning\n\nI expect you to actively participate in class (that does not mean you have to speak to the entire group)\nLots of hands-on activities and discussions\nWe will be using Gradescope (more on this later) to submit in-class work and for homework assignments"
  },
  {
    "objectID": "slides-01-01.html#ice-breaker",
    "href": "slides-01-01.html#ice-breaker",
    "title": "Introduction",
    "section": "Ice Breaker",
    "text": "Ice Breaker\n\nFind at least one person to talk to.\nIntroduce yourself (name, major/program, anything else you’d like to share)\nWhat is something you are good at and that took many hours of practice?"
  },
  {
    "objectID": "slides-01-01.html#course-overview",
    "href": "slides-01-01.html#course-overview",
    "title": "Introduction",
    "section": "Course Overview",
    "text": "Course Overview\n\nIntroduction to advanced techniques in Data Science\nFocus on both concepts and examples\nFocus on hands-on, bring a laptop to class if you have one\nFocus on real data in assignments"
  },
  {
    "objectID": "slides-01-01.html#course-overview-1",
    "href": "slides-01-01.html#course-overview-1",
    "title": "Introduction",
    "section": "Course Overview",
    "text": "Course Overview\nWhat is missing in this image?"
  },
  {
    "objectID": "slides-01-01.html#course-overview-2",
    "href": "slides-01-01.html#course-overview-2",
    "title": "Introduction",
    "section": "Course Overview",
    "text": "Course Overview\nTopics:\n\nReview the cycle of data science: data wrangling, and visualization\nMajor topics: how to make data-driven inferences and decisions by using fundamental techniques in machine learning and neural networks"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSIT-360-557 Syllabus",
    "section": "",
    "text": "Course Information\n\nMonday and Wednesday, 12:00 – 1:25pm\nMode: in-person, at School of Business 011\n\n\n\nCourse Description\nThis course provides the advanced techniques in Data Science. After learning data manipulating, processing, cleaning, and visualization, the major goals of this course are to learn how to make data-driven inferences and decisions by using fundamental techniques in machine learning, in Python language.\n\n\nInstructor\n\nAdriana Picoral, PhD\nOffice: Schmitt 374\nemail: picorala@montclair.edu\nOffice Hours (open door/drop in):\n\nTuesdays and Thursdays, 1:45pm to 3:30pm\n\n\n\n\nCourse Outcomes\nUpon completion of the course, students should be able to:\n\nCO1: understand the concepts and various phases of Data Science\nCO2: understand how to use Python data science libraries to demonstrate the data science skills and visualize data\nCO3: understand the concepts of fundamental techniques in machine learning and neural networks\nCO4: implement techniques of machine learning algorithms on data analysis using advanced libraries such as sci-kit learn and TensorFlow in Python\nCO5: apply data science concepts and skills to solve problems with real-world data sets\n\n\n\nCourse Schedule\n\n\n\n\n\n\nWeek\nStart Date\nTopic\n\n\n\n\n01\nJanuary 22\nIntroduction\n\n\n02\nJanuary 29\nPython Review\n\n\n03\nFeb 05\nNumpy & Pandas\n\n\n04\nFebruary 12\nData Wrangling\n\n\n05\nFebruary 19\nSupervised Learning\n\n\n06\nFebruary 26\nSupervised Learning\n\n\n07\nMarch 05\nSupervised Learning\n\n\n08\nMarch 17\nSupervised Learning\n\n\n09\nMarch 24\nSupervised Learning\n\n\n10\nMarch 31\nUnsupervised Learning\n\n\n11\nApril 07\nUnsupervised Learning\n\n\n12\nApril 14\nUnsupervised Learning\n\n\n13\nApril 21\nOther Advanced Techniques\n\n\n14\nApril 28\nFinal Project Presentations\n\n\n\n\n\n\n\n\nGrading Breakdown\n\n\n\nAssessment Element\nPercentage of Final Grade\n\n\n\n\nIn-class Activities\n10%\n\n\nProjects\n30%\n\n\nQuizzes (5)\n30%\n\n\nGroup Project\n20%\n\n\nProject Presentation\n10%\n\n\n\n\n\nLate Work Policy\nStudents are expected to complete work on schedule. The late policy for all assignments is as follows:\n\n20% points off if submitted within 24 hours after the due date\n30% off if submitted 24-48 hours after the due date\nNo credit if submitted two days or more days after the due date unless prior arrangements are made with the instructor with acceptable reasons.\n\nNote: This is a firm policy and it will be automatically applied\n\n\nAttendance Policy\nAttendance is mandatory. All students must sign an attendance sheet at the beginning of class. Excused absences include illness or a serious personal crisis (a letter from the Dean of students is required). If you are missing class and have a reasonable excuse, contact me before lecture if you want to make up missed assessments.\n\n\nGrading Scale\n\n\n\nLetter Grade\nGrade Percentage\n\n\n\n\nA\n94-100%\n\n\nA-\n90-93%\n\n\nB+\n87-89%\n\n\nB\n84-86%\n\n\nB-\n80-83%\n\n\nC+\n77-79%\n\n\nC\n74-76%\n\n\nC-\n70-73%\n\n\nD+ (undergraduate only)\n67-69%\n\n\nD (undergraduate only)\n64-66%\n\n\nD- (undergraduate only)\n60-63%\n\n\nF\n0-59%\n\n\n\n\n\nAcademic Honesty and Integrity\nAcademic Honesty is a core University value. Take time to understand the University’s policy. Your questions about academic honesty are always welcome.\nAll work you submit for grading in this course must be your own. Submitting work that includes (minor and/or major) components that are not your own work is considered plagiarism. I recommend that when talking with others about the assignment, do not write anything down.\nKeep in mind that all assignments and practice problems provided in this course are meant to help you practice the skills that you will need to do well in all assessments (including on paper quizzes), so it is generally in your best interest to avoid taking shortcuts even on practice problems (which are ungraded).\nSharing your code with others (in addition to copying code from others) is considered a break of the academic integrity code (unauthorized assistance) as well.\n\n\nLand Acknowledgement\nWe respectfully acknowledge that Montclair State University occupies land in Lenapehoking, the traditional and expropriated territory of the Lenape. As a state institution, we recognize and support the sovereignty of New Jersey’s three state-recognized tribes: the Ramapough Lenape, Nanticoke Lenni-Lenape, and Powhatan Renape nations.\nWe recognize the sovereign nations of the Lenape diaspora elsewhere in North America, as well as other Indigenous individuals and communities now residing in New Jersey. By offering this land acknowledgement, we commit to addressing the historical legacies of Indigenous dispossession and dismantling practices of erasure that persist today. We recognize the resilience and persistence of contemporary Indigenous communities and their role in educating all of us about justice, equity, and the stewardship of the land throughout the generations.\n\n\nSubject to Change\nChanges may be made to this syllabus as needed to support student learning. Any updates will be announced in class or through course materials with advance notice."
  },
  {
    "objectID": "environment.html",
    "href": "environment.html",
    "title": "Coding Environment",
    "section": "",
    "text": "We will be applying the concepts in this course to hands-on coding applications using real world data. For this, you need to have Python 3.8+ installed in your computer (or a computer that you have access to). I also recommend using Visual Studio Code for your development environment. That is what I will be using during class demonstrations and lecture materials.\nAfter installing and opening VS code, go to the File menu option and then select the Open Folder... option. Open an empty folder that you created, and that you know the location of in your computer. Create a new file that ends with the .py extension. If you do not have Python install in your machine, VS code will prompt you to install it.\nFor the next step you will need to have bash in your machine. If you have a windows machine, you will have to install bash – I recommend you install git bash.\nNext step is to install sckit-learn, one of the packages that we will be working with in this course. To do this, you can open a bash terminal in your VS code and type the following:\npip install -U scikit-learn\nWait and read the terminal standard output to check if the installation was completed successfully."
  },
  {
    "objectID": "slides-01-01.html#coding-tools",
    "href": "slides-01-01.html#coding-tools",
    "title": "Introduction",
    "section": "Coding tools",
    "text": "Coding tools\n\nWe will be coding in Python for this class.\nFor next lecture (Monday) you are expected to have set up your coding environment."
  },
  {
    "objectID": "slides-01-01.html#what-is-machine-learning",
    "href": "slides-01-01.html#what-is-machine-learning",
    "title": "Introduction",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\nsubfield of computer science + statistics\nbuilding algorithms/models based on data (i.e. collection of examples of some phenomenon)"
  },
  {
    "objectID": "slides-01-01.html#types-of-machine-learning",
    "href": "slides-01-01.html#types-of-machine-learning",
    "title": "Introduction",
    "section": "Types of Machine Learning",
    "text": "Types of Machine Learning\n\nsupervised (labeled data)\nsemi-supervised (both labeled and unlabeled data)\nunsupervised (unlabeled data)\nreinforcement (reward system, with the goal of learning a policy)\n\nFor all of these, data is required"
  },
  {
    "objectID": "slides-01-02.html#tidy-data",
    "href": "slides-01-02.html#tidy-data",
    "title": "Data",
    "section": "Tidy Data",
    "text": "Tidy Data\nThe data we will be working with contains variables as columns and observations as rows"
  },
  {
    "objectID": "slides-01-02.html#matrices-and-vectors",
    "href": "slides-01-02.html#matrices-and-vectors",
    "title": "Data",
    "section": "Matrices and Vectors",
    "text": "Matrices and Vectors\nFor machine learning, we have a feature matrix that contains all of our predictor variables, and a target vector that contains the label or target for each observation."
  },
  {
    "objectID": "slides-01-02.html#what-is-machine-learning",
    "href": "slides-01-02.html#what-is-machine-learning",
    "title": "Advanced Methods in Data Science",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\nsubfield of computer science + statistics\nbuilding algorithms/models based on data (i.e. collection of examples of some phenomenon)"
  },
  {
    "objectID": "slides-01-02.html#types-of-machine-learning",
    "href": "slides-01-02.html#types-of-machine-learning",
    "title": "Advanced Methods in Data Science",
    "section": "Types of Machine Learning",
    "text": "Types of Machine Learning\n\nsupervised (labeled data)\nsemi-supervised (both labeled and unlabeled data)\nunsupervised (unlabeled data)\nreinforcement (reward system, with the goal of learning a policy)\n\nFor all of these, data is required – one important step is feature engineering (how to transform data into feature – more on this later)\nDepending on the target type, classification or regression"
  },
  {
    "objectID": "slides-01-02.html#supervised-learning",
    "href": "slides-01-02.html#supervised-learning",
    "title": "Advanced Methods in Data Science",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nLabeled data – input (features), output (target, response)\n\nCan you think of examples of supervised learning?"
  },
  {
    "objectID": "slides-01-02.html#supervised-learning-1",
    "href": "slides-01-02.html#supervised-learning-1",
    "title": "Advanced Methods in Data Science",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nExample: spam vs. not spam\n\nWhat features can we use?"
  },
  {
    "objectID": "slides-01-02.html#supervised-learning-algorithms",
    "href": "slides-01-02.html#supervised-learning-algorithms",
    "title": "Advanced Methods in Data Science",
    "section": "Supervised Learning Algorithms",
    "text": "Supervised Learning Algorithms\n\nSupport Vector Machines (SVM)\nDecision Tree/Random Forest\nLogistic Regression\nLinear Regression\nNaive Bayes\nK-Nearest Neighbors\n\nHow to decide?"
  },
  {
    "objectID": "slides-01-02.html#unsupervised-learning",
    "href": "slides-01-02.html#unsupervised-learning",
    "title": "Advanced Methods in Data Science",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nClustering (K-Means)\nPrincipal Component Analysis (Dimensionality Reduction)"
  },
  {
    "objectID": "slides-01-02.html#reinforcement-learning",
    "href": "slides-01-02.html#reinforcement-learning",
    "title": "Advanced Methods in Data Science",
    "section": "Reinforcement Learning",
    "text": "Reinforcement Learning\n\nRecommendation Systems (users give feed on whether a recommendation was good or not)\nAutomated Robots\nAutonomous Driving\nNatural Language Processing – text prediction, translation"
  },
  {
    "objectID": "slides-01-03.html#data",
    "href": "slides-01-03.html#data",
    "title": "Data",
    "section": "Data",
    "text": "Data\nThe data we will be working with contains variables as columns and observations as rows (often called tidy data)\nFeature engineering: data –&gt; features\nRemember the spam vs not spam example? How do we go from words to features?"
  },
  {
    "objectID": "slides-01-03.html#data-quality",
    "href": "slides-01-03.html#data-quality",
    "title": "Data",
    "section": "Data quality",
    "text": "Data quality\nWhat to look for:\n\na data dictionary\ninformation on how the data were collected"
  },
  {
    "objectID": "slides-01-03.html#data-format",
    "href": "slides-01-03.html#data-format",
    "title": "Data",
    "section": "Data format",
    "text": "Data format\n\ntabular – tables, rows and columns\nhierachical – values are nested (like a tree)\nunstructured data – no structure, for example: emails, videos, pictures"
  },
  {
    "objectID": "slides-01-03.html#tabular-data",
    "href": "slides-01-03.html#tabular-data",
    "title": "Data",
    "section": "Tabular Data",
    "text": "Tabular Data\nRows and Columns\n\n\n\nDay\nHigh\nLow\nWind\nForecast\n\n\n\n\nTuesday\n24\n15\n0 to 15 mph\nSunny\n\n\nWednesday\n38\n17\n5 to 15 mph\nMostly Sunny\n\n\nThursday\n34\n13\n5 to 15 mph\nMostly Sunny"
  },
  {
    "objectID": "slides-01-03.html#hierachical-data",
    "href": "slides-01-03.html#hierachical-data",
    "title": "Data",
    "section": "Hierachical Data",
    "text": "Hierachical Data\nTuesday:\n   ↳ Temperature:\n      ↳ Low: 15\n      ↳ High: 24\n   ↳ Wind:\n      ↳ Speed: 0 to 15 mph \n      ↳ Direction: West\nWednesday:\n   ↳ Temperature:\n      ↳ Low: 17\n      ↳ High: 38\n   ↳ Wind:\n      ↳ Speed: 5 to 15 mph\n      ↳ Direction: North West"
  },
  {
    "objectID": "slides-01-03.html#unstructured-data",
    "href": "slides-01-03.html#unstructured-data",
    "title": "Data",
    "section": "Unstructured Data",
    "text": "Unstructured Data\nOne winter, I became very quiet\nand saw my life. It was February\n\nand outside in the city streets,\nsnow fell but would not collect.\n\nI bought snapdragons and thistle,\ngot some discount peach roses\n\nthat smelled off. I split them\nbetween vases and moved\n\nthe bouquets from room to room\nwhile a violin solo rang out.\nfull poem"
  },
  {
    "objectID": "slides-01-03.html#matrices-and-vectors",
    "href": "slides-01-03.html#matrices-and-vectors",
    "title": "Data",
    "section": "Matrices and Vectors",
    "text": "Matrices and Vectors\nFor machine learning, we have a feature matrix that contains all of our predictor variables, and a target vector that contains the label or target for each observation."
  },
  {
    "objectID": "slides-01-03.html#training-validation-and-testing",
    "href": "slides-01-03.html#training-validation-and-testing",
    "title": "Data",
    "section": "Training, Validation and Testing",
    "text": "Training, Validation and Testing\nWe usually need enough data to split it into training and validation (and testing)\n\nMost of our data will be used to build our model (training)\nWe never predict data that was in our training data set\nOur validation data set is used to fine tune our model (we can also do cross-validation)\nThe test data set is used to assess the final mode that has been selected during the validation process\n\nThe goal is to not overfit our data to our model"
  },
  {
    "objectID": "slides-01-02.html",
    "href": "slides-01-02.html",
    "title": "Advanced Methods in Data Science",
    "section": "",
    "text": "What is it? (discuss)\n\n\n\nsubfield of computer science + statistics\nbuilding algorithms/models based on data (i.e. collection of examples of some phenomenon)\n\n\n\n\n\nsupervised (labeled data) – classification and regression\nsemi-supervised (both labeled and unlabeled data)\nunsupervised (unlabeled data)\nreinforcement (reward system, with the goal of learning a policy)\n\nFor all of these, data is required – one important step is feature engineering (how to transform data into feature – more on this later)\n\n\n\n\nLabeled data – input (features), output (target, response)\n\nCan you think of examples of supervised learning?\n\n\n\n\nExample: spam vs. not spam\n\nWhat features can we use?\n\n\n\n\nSupport Vector Machines (SVM)\nDecision Tree/Random Forest\nLogistic Regression\nLinear Regression\nNaive Bayes\nK-Nearest Neighbors\n\nHow to decide?\n\n\n\n\nClustering (K-Means)\nPrincipal Component Analysis (Dimensionality Reduction)\n\n\n\n\n\nRecommendation Systems (users give feed on whether a recommendation was good or not)\nAutomated Robots\nAutonomous Driving\nNatural Language Processing – text prediction, translation"
  },
  {
    "objectID": "slides-01-01.html",
    "href": "slides-01-01.html",
    "title": "Introduction",
    "section": "",
    "text": "Adriana Picoral (PhD, you can call me Adriana)\npicorala@montclair.edu\nOffice:\nOffice hours: Tuesdays and Thursdays, 1:45pm to 3:30pm"
  },
  {
    "objectID": "environment.html#trouble-shooting",
    "href": "environment.html#trouble-shooting",
    "title": "Coding Environment",
    "section": "Trouble Shooting",
    "text": "Trouble Shooting\n\nRun where python and which python to find which Python installation your terminal has associated with different python aliases.\nYou can install a package to a specific python installation by running something like this: /usr/local/bin/python3 -m pip install statsmodels"
  },
  {
    "objectID": "environment.html#troubleshooting",
    "href": "environment.html#troubleshooting",
    "title": "Coding Environment",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nRun where python and which python to find which Python installation your terminal has associated with different python aliases.\nYou can install a package to a specific python installation by running something like this: /usr/local/bin/python3 -m pip install statsmodels"
  },
  {
    "objectID": "modules.html#assignments",
    "href": "modules.html#assignments",
    "title": "Modules",
    "section": "",
    "text": "Beginning of semester survey"
  },
  {
    "objectID": "slides-01-03.html#practice",
    "href": "slides-01-03.html#practice",
    "title": "Data",
    "section": "Practice",
    "text": "Practice\n\nAccess this kaggle dataset on house prices\nWhat are the variables in the data? (is there a data dictionary?)\nWhat is this data set from? What’s its source?\nAny problems you see with it?\nWhich variable could we use as target (response)?\nWhich variables would you use as features? Any feature engineering you can think of?"
  },
  {
    "objectID": "modules.html#video-lessons",
    "href": "modules.html#video-lessons",
    "title": "Modules",
    "section": "Video Lessons",
    "text": "Video Lessons\n\nOpening data files in VS Code Passcode: Uu5.@XE*"
  },
  {
    "objectID": "pandas-reference.html",
    "href": "pandas-reference.html",
    "title": "Pandas – reference sheet",
    "section": "",
    "text": "Import the package\n\nimport pandas\n\nYou can provide a shorter alias, which makes it easier to type\n\nimport pandas as pd\n\n\n\nLoad Data"
  },
  {
    "objectID": "slides-02-01.html",
    "href": "slides-02-01.html",
    "title": "Intro to Pandas",
    "section": "",
    "text": "Open a folder (that you have created, with the data we are going to be using) in VS code"
  },
  {
    "objectID": "slides-02-01.html#import-the-package",
    "href": "slides-02-01.html#import-the-package",
    "title": "Intro to Pandas",
    "section": "Import the package",
    "text": "Import the package\n\nimport pandas\n\nYou can provide a shorter alias, which makes it easier to type\n\nimport pandas as pd"
  },
  {
    "objectID": "slides-02-01.html#load-data",
    "href": "slides-02-01.html#load-data",
    "title": "Intro to Pandas",
    "section": "Load Data",
    "text": "Load Data\nLet’s use this kaggle dataset on house prices as an example. I downloaded the data and saved it in a folder called data.\n\ndata_frame = pd.read_csv(\"data/US houuse price of 10 states.csv\")"
  },
  {
    "objectID": "slides-02-01.html#inspect-the-data",
    "href": "slides-02-01.html#inspect-the-data",
    "title": "Intro to Pandas",
    "section": "Inspect the data",
    "text": "Inspect the data\n\ndata_frame.head()\ndata_frame.info()\ndata_frame.describe()\ndata_frame.shape\ndata_frame.columns\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 12075 entries, 0 to 12074\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   date        12075 non-null  object\n 1   house_size  11107 non-null  object\n 2   bed         11410 non-null  object\n 3   bath        11410 non-null  object\n 4   price       11009 non-null  object\n 5   broker      9569 non-null   object\n 6   street      12075 non-null  object\n 7   city        12075 non-null  object\n 8   state_name  12075 non-null  object\n 9   zip_code    12075 non-null  int64 \ndtypes: int64(1), object(9)\nmemory usage: 943.5+ KB\n\n\nIndex(['date', 'house_size', 'bed', 'bath', 'price', 'broker', 'street',\n       'city', 'state_name', 'zip_code'],\n      dtype='object')"
  },
  {
    "objectID": "slides-02-01.html#setting-up-your-coding-environment",
    "href": "slides-02-01.html#setting-up-your-coding-environment",
    "title": "Intro to Pandas",
    "section": "Setting up your coding environment",
    "text": "Setting up your coding environment\nOpen a folder (that you have created, with the data we are going to be using) in VS code"
  },
  {
    "objectID": "slides-02-01.html#inspect-variables",
    "href": "slides-02-01.html#inspect-variables",
    "title": "Intro to Pandas",
    "section": "Inspect variables",
    "text": "Inspect variables\n\ndata_frame[\"bed\"].head()\ndata_frame[[\"bed\", \"bath\"]].head()\n\n\n\n\n\n\n\n\nbed\nbath\n\n\n\n\n0\n4bd\n4bd\n\n\n1\n2bd\n2bd\n\n\n2\n3bd\n3bd\n\n\n3\nStudio\nStudio\n\n\n4\n3bd\n3bd"
  },
  {
    "objectID": "slides-02-01.html#inspect-variables-1",
    "href": "slides-02-01.html#inspect-variables-1",
    "title": "Intro to Pandas",
    "section": "Inspect variables",
    "text": "Inspect variables\n\ndata_frame[\"bed\"].count() # number of non-null values\ndata_frame[\"bed\"].nunique() # count of unique values\ndata_frame[\"bed\"].value_counts() # count for each unique value\n\nbed\n3bd       5172\n4bd       2998\n2bd       1797\n5bd        762\n1bd        290\nStudio     153\n6bd        150\n7bd         31\n8bd         31\n9bd          9\n12bd         4\n10bd         3\n14bd         2\n21bd         2\n15bd         2\n11bd         2\n16bd         1\n13bd         1\nName: count, dtype: int64"
  },
  {
    "objectID": "slides-02-01.html#pandas.series.str",
    "href": "slides-02-01.html#pandas.series.str",
    "title": "Intro to Pandas",
    "section": "pandas.Series.str",
    "text": "pandas.Series.str\nA pandas.Series is one column in our data frame\nRead the documentation on pandas.Series.str – how can we create a numeric variable based on the \"bed\" column?"
  },
  {
    "objectID": "modules.html#video-lesson",
    "href": "modules.html#video-lesson",
    "title": "Modules",
    "section": "Video Lesson",
    "text": "Video Lesson\n\nOpening data files in VS Code Passcode: Uu5.@XE*"
  },
  {
    "objectID": "modules.html#slides-1",
    "href": "modules.html#slides-1",
    "title": "Modules",
    "section": "Slides",
    "text": "Slides\n\nIntro to Pandas\nIntro to Seaborn\nRegular Expressions"
  },
  {
    "objectID": "slides-02-02.html#set-up-your-coding-environment",
    "href": "slides-02-02.html#set-up-your-coding-environment",
    "title": "Intro to Seaborn",
    "section": "Set up your coding environment",
    "text": "Set up your coding environment\n\nCreate a new .ipynb in your VS code folder.\nYou might need to run pip install notebook (python3 -m pip install notebook)\nYou also might need to run pip install jupyterlab"
  },
  {
    "objectID": "slides-02-02.html#import-the-libraries-we-will-be-using",
    "href": "slides-02-02.html#import-the-libraries-we-will-be-using",
    "title": "Intro to Seaborn",
    "section": "Import the libraries we will be using",
    "text": "Import the libraries we will be using\n\nimport seaborn as sns\n\nMake sure you import pandas and read the data in:\n\nimport pandas as pd\ndata_frame = pd.read_csv(\"data/US houuse price of 10 states.csv\")"
  },
  {
    "objectID": "slides-02-02.html#scatter-plot",
    "href": "slides-02-02.html#scatter-plot",
    "title": "Intro to Seaborn",
    "section": "Scatter plot",
    "text": "Scatter plot\n\nsns.scatterplot(data = data_frame, x = \"bed_numeric\", y = \"price_numeric\")"
  },
  {
    "objectID": "slides-02-02.html",
    "href": "slides-02-02.html",
    "title": "Intro to Seaborn",
    "section": "",
    "text": "Create a new .ipynb in your VS code folder.\nYou might need to run pip install notebook (python3 -m pip install notebook)\nYou also might need to run pip install jupyterlab"
  },
  {
    "objectID": "slides-02-01.html#filtering-the-data",
    "href": "slides-02-01.html#filtering-the-data",
    "title": "Intro to Pandas",
    "section": "Filtering the data",
    "text": "Filtering the data\n\ndata_frame[data_frame[\"bed\"] == \"3bd\"]\n\n\n\n\n\n\n\n\ndate\nhouse_size\nbed\nbath\nprice\nbroker\nstreet\ncity\nstate_name\nzip_code\n\n\n\n\n2\nAUG 29, 2024\n1,926 sqft (on 0.45 acres)\n3bd\n3bd\n$375,000\nColdwell Banker Hartung\n6761 Landover Cir\nTallahassee\nFlorida\n32317\n\n\n4\nAUG 29, 2024\n1,205 sqft\n3bd\n3bd\n$233,900\nD R Horton Realty of NW Florida, LLC\n6274 June Bug Dr\nMilton\nFlorida\n32583\n\n\n14\nAUG 28, 2024\n1,820 sqft\n3bd\n3bd\n$330,500\nEXP Realty, LLC\n8564 Westview Ln\nPensacola\nFlorida\n32514\n\n\n15\nAUG 28, 2024\n1,370 sqft\n3bd\n3bd\n$173,000\nBetter Homes And Gardens Real Estate Main Stre...\n6905 Woodley Dr\nPensacola\nFlorida\n32503\n\n\n17\nAUG 28, 2024\n2,681 sqft (on 1.82 acres)\n3bd\n3bd\n$525,000\nAmerican Valor Realty LLC\n8021 Quiet Dr\nPensacola\nFlorida\n32526\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n12059\nAUG 30, 2024\n2,610 sqft\n3bd\n3bd\n$1,143,909\nBALBOA REAL ESTATE, INC.\n50525 Spyglass Hill Dr\nLa Quinta\nCA\n92253\n\n\n12063\nAUG 30, 2024\n1,983 sqft (on 1 acre)\n3bd\n3bd\n$1,385,000\nCompass\n10241 McBroom St\nSunland\nCA\n91040\n\n\n12065\nAUG 30, 2024\n1,757 sqft\n3bd\n3bd\n$568,000\nBerkshire Hathaway Home Serv.\n26848 Hanford St\nMenifee\nCA\n92584\n\n\n12069\nAUG 30, 2024\n2,000 sqft\n3bd\n3bd\n$1,880,000\nReal Estate Legends USA\n16 Riveroaks\nIrvine\nCA\n92602\n\n\n12074\nAUG 30, 2024\n1,615 sqft\n3bd\n3bd\n$508,000\nStarlitloan&Realty\n1668 Ravenswood Rd\nBeaumont\nCA\n92223\n\n\n\n\n5172 rows × 10 columns"
  },
  {
    "objectID": "slides-02-01.html#handling-missing-data-drop-missing-data",
    "href": "slides-02-01.html#handling-missing-data-drop-missing-data",
    "title": "Intro to Pandas",
    "section": "Handling Missing Data – drop missing data",
    "text": "Handling Missing Data – drop missing data\nDocumentation on .dropna()\n\ndata_frame.dropna()\n\n\n\n\n\n\n\n\ndate\nhouse_size\nbed\nbath\nprice\nbroker\nstreet\ncity\nstate_name\nzip_code\n\n\n\n\n2\nAUG 29, 2024\n1,926 sqft (on 0.45 acres)\n3bd\n3bd\n$375,000\nColdwell Banker Hartung\n6761 Landover Cir\nTallahassee\nFlorida\n32317\n\n\n3\nAUG 29, 2024\n1,132 sqft\nStudio\nStudio\n$190,000\nEXP Realty, LLC\n1701 S Fairfield Dr\nPerdido Key\nFlorida\n32507\n\n\n4\nAUG 29, 2024\n1,205 sqft\n3bd\n3bd\n$233,900\nD R Horton Realty of NW Florida, LLC\n6274 June Bug Dr\nMilton\nFlorida\n32583\n\n\n5\nAUG 29, 2024\n3,044 sqft (on 0.34 acres)\n4bd\n4bd\n$416,402\nADAMS HOME REALTY, INC\n6528 Benelli Dr\nMilton\nFlorida\n32570\n\n\n13\nAUG 28, 2024\n1,254 sqft\n2bd\n2bd\n$250,000\nJANET COULTER REALTY\n520 Richard Jackson Blvd #2810\nPanama City Beach\nFlorida\n32407\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n12067\nAUG 30, 2024\n2,352 sqft\n4bd\n4bd\n$795,000\nRealty Masters\n12549 Navel Ct\nRiverside\nCA\n92503\n\n\n12069\nAUG 30, 2024\n2,000 sqft\n3bd\n3bd\n$1,880,000\nReal Estate Legends USA\n16 Riveroaks\nIrvine\nCA\n92602\n\n\n12070\nAUG 30, 2024\n3,835 sqft\n5bd\n5bd\n$1,935,000\nRealty ONE Group Pacific\n1154 Via Vera Cruz\nSan Marcos\nCA\n92078\n\n\n12072\nAUG 30, 2024\n2,616 sqft\n4bd\n4bd\n$685,000\nAnderson Real Estate Group\n5509 W Modoc Avenue\nVisalia\nCA\n93291\n\n\n12074\nAUG 30, 2024\n1,615 sqft\n3bd\n3bd\n$508,000\nStarlitloan&Realty\n1668 Ravenswood Rd\nBeaumont\nCA\n92223\n\n\n\n\n8068 rows × 10 columns"
  },
  {
    "objectID": "slides-02-01.html#handling-missing-data-fill-missing-data",
    "href": "slides-02-01.html#handling-missing-data-fill-missing-data",
    "title": "Intro to Pandas",
    "section": "Handling Missing Data – fill missing data",
    "text": "Handling Missing Data – fill missing data\nDocumentation on .fillna()\n\ndata_frame.fillna(0)\ndata_frame.ffill()\ndata_frame.bfill()\ndata_frame.fillna(data_frame.mean())\ndata_frame[\"bed_numeric\"].fillna(data_frame[\"bed_numeric\"].mean())"
  },
  {
    "objectID": "slides-02-03.html#regex",
    "href": "slides-02-03.html#regex",
    "title": "Regular Expressions",
    "section": "Regex",
    "text": "Regex\nRegular expressions are:\n\npatterns used to match and manipulate text strings"
  },
  {
    "objectID": "slides-02-03.html#other-syntax",
    "href": "slides-02-03.html#other-syntax",
    "title": "Regular Expressions",
    "section": "Other syntax",
    "text": "Other syntax\n\n\\d - matches any digit (0-9)\n\\w - matches any word character (a-z, A-Z, 0-9, _)\n\\s - matches any whitespace character (space, tab, newline)\n[] used to indicate a set of characters.\n{m} matches m times\n{m,n} matches m to n times\n\ne.g., lower case letters: [a-z], upper case letters: [A-Z]"
  },
  {
    "objectID": "slides-02-03.html#python-examples",
    "href": "slides-02-03.html#python-examples",
    "title": "Regular Expressions",
    "section": "Python examples",
    "text": "Python examples\nEmail validation pattern:\n\npattern = r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$'\n\nPhone number pattern (US format):\n\npattern = r'\\d{3}-\\d{3}-\\d{4}'"
  },
  {
    "objectID": "slides-02-03.html#basic-patterns",
    "href": "slides-02-03.html#basic-patterns",
    "title": "Regular Expressions",
    "section": "Basic Patterns",
    "text": "Basic Patterns\n\nLiteral characters match themselves (“cat” matches “cat”)\nSpecial characters have specific meanings:\n\n. (dot) - matches any single character except newline\n* matches 0 or more of the previous character\n+ matches 1 or more of the previous character\n? - matches 0 or 1 of the previous character\n^ - matches start of line\n$ - matches end of line"
  },
  {
    "objectID": "slides-02-03.html#examples",
    "href": "slides-02-03.html#examples",
    "title": "Regular Expressions",
    "section": "Examples",
    "text": "Examples\n[abc] - matches any single character from the set (a, b, or c)\n[^abc] - matches any single character NOT in the set"
  },
  {
    "objectID": "modules.html#practice",
    "href": "modules.html#practice",
    "title": "Modules",
    "section": "Practice",
    "text": "Practice\nClean this data set on Titanic survivors"
  },
  {
    "objectID": "slides-02-04.html",
    "href": "slides-02-04.html",
    "title": "Numpy",
    "section": "",
    "text": "import numpy as np"
  },
  {
    "objectID": "slides-02-04.html#package-and-data",
    "href": "slides-02-04.html#package-and-data",
    "title": "Numpy",
    "section": "Package and Data",
    "text": "Package and Data\n\nimport numpy as np"
  },
  {
    "objectID": "slides-02-04.html#recode-data",
    "href": "slides-02-04.html#recode-data",
    "title": "Numpy",
    "section": "Recode Data",
    "text": "Recode Data\n\nbank['sex'] = np.where(bank['sex'] == \"Female\", 0, 1)\nbank.head()"
  },
  {
    "objectID": "slides-02-04.html#transform-data",
    "href": "slides-02-04.html#transform-data",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\n\nbank['log_salary'] = np.log(bank['salary'])"
  },
  {
    "objectID": "slides-03-01.html#methods-to-use",
    "href": "slides-03-01.html#methods-to-use",
    "title": "Data Wrangling",
    "section": "Methods to use",
    "text": "Methods to use\nTwo .replace() methods, one from pandas.Series.str and another from pandas.DataFrame\n\ndata_frame[column_name] = data_frame[column_name].str.replace(r\"none\", \"0\")\ndata_frame[column_name] = pd.to_numeric(data_frame[column_name])\ndata_frame = data_frame.replace({'data_frame': {'value1': '0', 'value2': '1'}})"
  },
  {
    "objectID": "slides-03-01.html#practice",
    "href": "slides-03-01.html#practice",
    "title": "Data Wrangling",
    "section": "Practice",
    "text": "Practice\n\nClean this data set on Titanic survivors\nMake sure you define a main() methods that reads and writes the data\nJoin Gradescope – it’s free, and join our course (Entry Code:8XG2NV)\nSubmit your file(s) to the Data Wrangling assignment"
  },
  {
    "objectID": "slides-03-01.html#exploratory-data-analysis",
    "href": "slides-03-01.html#exploratory-data-analysis",
    "title": "Data Wrangling",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nCalculating descriptive stats of a numeric variable by a group.\n\ndata_frame.groupby(\"category\")[\"numeric_value\"].mean().reset_index()\ndata_frame.groupby(\"category\")[\"numeric_value\"].agg([\"mean\", \"std\", \"max\", \"min\"]).reset_index()"
  },
  {
    "objectID": "slides-03-01.html#bar-plots",
    "href": "slides-03-01.html#bar-plots",
    "title": "Data Wrangling",
    "section": "Bar plots",
    "text": "Bar plots\n\nimport seaborn as sns\n\nsns.barplot(data = desc_stats, x = \"category\", y = \"mean\")"
  },
  {
    "objectID": "modules.html#slides-2",
    "href": "modules.html#slides-2",
    "title": "Modules",
    "section": "Slides",
    "text": "Slides\n\nData Wrangling\nNumpy\nData Modeling – intro"
  },
  {
    "objectID": "slides-03-02.html#data-modeling",
    "href": "slides-03-02.html#data-modeling",
    "title": "Intro to Data Modeling",
    "section": "Data Modeling",
    "text": "Data Modeling"
  },
  {
    "objectID": "slides-03-02.html#inference-vs.-prediction",
    "href": "slides-03-02.html#inference-vs.-prediction",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\nInference:\n\nunderstanding relationships and testing hypotheses about how variables interact\nexplain the underlying mechanisms and relationships in the data\nHow does education relates to income?"
  },
  {
    "objectID": "slides-03-02.html#inference-vs.-prediction-1",
    "href": "slides-03-02.html#inference-vs.-prediction-1",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\nPrediction:\n\ngenerating accurate estimates of future or unknown values\noptimizing the model’s ability to make correct predictions\nAccurately forecast someone’s income based on their education level."
  },
  {
    "objectID": "slides-03-02.html#inference-vs.-prediction-2",
    "href": "slides-03-02.html#inference-vs.-prediction-2",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\nInference models prioritize interpretability and often use simpler, more transparent methods like linear regression\nPrediction models may use more complex “black box” approaches like neural networks if they improve accuracy"
  },
  {
    "objectID": "slides-03-02.html#inference-vs.-prediction-3",
    "href": "slides-03-02.html#inference-vs.-prediction-3",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\nIn inference, you carefully choose variables based on theory and prior research\nFor prediction, you might include any feature that improves predictive performance, even if the relationship isn’t theoretically clear (! careful with this approach)"
  },
  {
    "objectID": "slides-03-02.html#inference-vs.-prediction-4",
    "href": "slides-03-02.html#inference-vs.-prediction-4",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\nInference focuses on metrics like p-values, confidence intervals, and effect sizes\nPrediction emphasizes metrics like mean squared error, classification accuracy, or the area under the ROC curve (AUC)"
  },
  {
    "objectID": "slides-03-02.html#inference-vs.-prediction-5",
    "href": "slides-03-02.html#inference-vs.-prediction-5",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\nOverfitting concerns\n\nboth approaches need to address overfitting\nit’s especially critical in prediction\ninference models might accept slightly worse fit for better interpretability\nprediction models focus heavily on cross-validation and out-of-sample performance"
  },
  {
    "objectID": "slides-03-02.html#inference",
    "href": "slides-03-02.html#inference",
    "title": "Intro to Data Modeling",
    "section": "Inference",
    "text": "Inference\nFor interpreting statistical models, we will be using the statsmodels Python package"
  },
  {
    "objectID": "slides-03-02.html#ordinary-least-squares-ols-regression",
    "href": "slides-03-02.html#ordinary-least-squares-ols-regression",
    "title": "Intro to Data Modeling",
    "section": "Ordinary Least Squares (OLS) regression",
    "text": "Ordinary Least Squares (OLS) regression\nOLS is a method that finds the best-fitting straight line through a set of points by minimizing the sum of squared vertical distances between the data points and the line.\nThe goal is to find a line y = β₀ + β₁x that best fits your data, where:\n\nβ₀ is the y-intercept\nβ₁ is the slope\nx is your independent variable – or target\ny is your dependent variable – or feature(s)"
  },
  {
    "objectID": "slides-03-02.html#ordinary-least-squares-ols-regression-1",
    "href": "slides-03-02.html#ordinary-least-squares-ols-regression-1",
    "title": "Intro to Data Modeling",
    "section": "Ordinary Least Squares (OLS) regression",
    "text": "Ordinary Least Squares (OLS) regression\nFor each data point, OLS:\n\nCalculates the vertical distance (residual) between the actual y-value and the predicted y-value\nSquares these distances (to make negatives positive and penalize larger errors more)\nSums all squared distances\nFinds the line that minimizes this sum"
  },
  {
    "objectID": "slides-03-02.html#ordinary-least-squares-ols-regression-2",
    "href": "slides-03-02.html#ordinary-least-squares-ols-regression-2",
    "title": "Intro to Data Modeling",
    "section": "Ordinary Least Squares (OLS) regression",
    "text": "Ordinary Least Squares (OLS) regression\nKey Assumptions:\n\nLinear relationship between variables\nIndependent observations\nHomoscedasticity (constant variance in errors)\nNormally distributed errors\nNo perfect multicollinearity"
  },
  {
    "objectID": "slides-03-02.html#ordinary-least-squares-ols-regression-3",
    "href": "slides-03-02.html#ordinary-least-squares-ols-regression-3",
    "title": "Intro to Data Modeling",
    "section": "Ordinary Least Squares (OLS) regression",
    "text": "Ordinary Least Squares (OLS) regression\nAdvantages:\n\nSimple to understand and implement\nBest Linear Unbiased Estimator (BLUE) under certain conditions\nComputationally efficient\nClear interpretation of results"
  },
  {
    "objectID": "slides-03-02.html#ordinary-least-squares-ols-regression-4",
    "href": "slides-03-02.html#ordinary-least-squares-ols-regression-4",
    "title": "Intro to Data Modeling",
    "section": "Ordinary Least Squares (OLS) regression",
    "text": "Ordinary Least Squares (OLS) regression\nLimitations:\n\nSensitive to outliers\nAssumes linear relationships\nMay not capture complex patterns\nAll predictors must be independent"
  },
  {
    "objectID": "slides-03-02.html#ols-regression-in-statsmodels",
    "href": "slides-03-02.html#ols-regression-in-statsmodels",
    "title": "Intro to Data Modeling",
    "section": "OLS regression in statsmodels",
    "text": "OLS regression in statsmodels\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n\ndef main():\n    data = pd.read_csv(\"data/clean_titanic.csv\")\n    X = data[[\"pclass\", \"sex\"]]  \n    y = data[\"survived\"] \n\n    # Ordinary Least Squares (regression)\n    result = sm.OLS(y, sm.add_constant(X)).fit()\n    print(result.summary())\n\nmain()"
  },
  {
    "objectID": "slides-03-02.html#ols-regression-results",
    "href": "slides-03-02.html#ols-regression-results",
    "title": "Intro to Data Modeling",
    "section": "OLS regression results",
    "text": "OLS regression results\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               survived   R-squared:                       0.366\nModel:                            OLS   Adj. R-squared:                  0.365\nMethod:                 Least Squares   F-statistic:                     255.2\nDate:                Tue, 11 Feb 2025   Prob (F-statistic):           3.19e-88\nTime:                        14:23:34   Log-Likelihood:                -417.77\nNo. Observations:                 887   AIC:                             841.5\nDf Residuals:                     884   BIC:                             855.9\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=============================================================================="
  },
  {
    "objectID": "slides-03-02.html#ols-regression-results-1",
    "href": "slides-03-02.html#ols-regression-results-1",
    "title": "Intro to Data Modeling",
    "section": "OLS regression results",
    "text": "OLS regression results\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.0825      0.040     26.795      0.000       1.003       1.162\npclass        -0.1577      0.016    -10.029      0.000      -0.189      -0.127\nsex           -0.5161      0.027    -18.776      0.000      -0.570      -0.462\n==============================================================================\nOmnibus:                       40.150   Durbin-Watson:                   1.921\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               44.755\nSkew:                           0.549   Prob(JB):                     1.91e-10\nKurtosis:                       3.060   Cond. No.                         9.07\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "slides-03-02.html#ols-regression-results-2",
    "href": "slides-03-02.html#ols-regression-results-2",
    "title": "Intro to Data Modeling",
    "section": "OLS regression results",
    "text": "OLS regression results\n\nWhat other variables can you add to the model?\nHow does the model change?"
  },
  {
    "objectID": "slides-03-02.html#ols-regression-results-3",
    "href": "slides-03-02.html#ols-regression-results-3",
    "title": "Intro to Data Modeling",
    "section": "OLS regression results",
    "text": "OLS regression results\nRun OLS regression on this kaggle dataset on house prices"
  },
  {
    "objectID": "slides-02-04.html#transform-data-1",
    "href": "slides-02-04.html#transform-data-1",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\nWhen do we log-transform numeric variables?\n\nNormalizing skewed distributions: Many real-world variables (like income, population, prices) tend to have right-skewed distributions, where most values are small but there are some very large values. Taking the logarithm can make these distributions more symmetric and closer to normal, which is beneficial for many statistical methods."
  },
  {
    "objectID": "slides-02-04.html#transform-data-2",
    "href": "slides-02-04.html#transform-data-2",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\nWhen do we log-transform numeric variables?\n\nManaging outliers: Log transformation reduces the impact of extreme values or outliers. For example, the difference between $1M and $2M becomes similar in magnitude to the difference between $10K and $20K when logged."
  },
  {
    "objectID": "slides-02-04.html#transform-data-3",
    "href": "slides-02-04.html#transform-data-3",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\nWhen do we log-transform numeric variables?\n\nLinearizing relationships: Many relationships that appear exponential become linear after log transformation. For instance, if \\(y = ax^b\\), then \\(log(y) = log(a) + b*log(x)\\) is linear. This makes the relationship easier to model and interpret."
  },
  {
    "objectID": "slides-02-04.html#transform-data-4",
    "href": "slides-02-04.html#transform-data-4",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\nWhen do we log-transform numeric variables?\n\nMaking multiplicative relationships additive: When variables have multiplicative effects (like compound growth), logging transforms them into additive relationships. This often makes more sense for how variables actually interact in real-world systems."
  },
  {
    "objectID": "slides-02-04.html#transform-data-5",
    "href": "slides-02-04.html#transform-data-5",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\nWhen do we log-transform numeric variables?\n\nStabilizing variance: When the variability of data increases with its magnitude (heteroscedasticity), log transformation can help create more constant variance across the range of values, which is important for many statistical methods."
  },
  {
    "objectID": "slides-03-02.html#package-and-data",
    "href": "slides-03-02.html#package-and-data",
    "title": "Numpy",
    "section": "Package and Data",
    "text": "Package and Data\n\nimport numpy as np"
  },
  {
    "objectID": "slides-03-02.html#recode-data",
    "href": "slides-03-02.html#recode-data",
    "title": "Numpy",
    "section": "Recode Data",
    "text": "Recode Data\n\nbank['sex'] = np.where(bank['sex'] == \"Female\", 0, 1)\nbank.head()"
  },
  {
    "objectID": "slides-03-02.html#transform-data",
    "href": "slides-03-02.html#transform-data",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\n\nbank['log_salary'] = np.log(bank['salary'])"
  },
  {
    "objectID": "slides-03-02.html#transform-data-1",
    "href": "slides-03-02.html#transform-data-1",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\nWhen do we log-transform numeric variables?\n\nNormalizing skewed distributions: Many real-world variables (like income, population, prices) tend to have right-skewed distributions, where most values are small but there are some very large values. Taking the logarithm can make these distributions more symmetric and closer to normal, which is beneficial for many statistical methods."
  },
  {
    "objectID": "slides-03-02.html#transform-data-2",
    "href": "slides-03-02.html#transform-data-2",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\nWhen do we log-transform numeric variables?\n\nManaging outliers: Log transformation reduces the impact of extreme values or outliers. For example, the difference between $1M and $2M becomes similar in magnitude to the difference between $10K and $20K when logged."
  },
  {
    "objectID": "slides-03-02.html#transform-data-3",
    "href": "slides-03-02.html#transform-data-3",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\nWhen do we log-transform numeric variables?\n\nLinearizing relationships: Many relationships that appear exponential become linear after log transformation. For instance, if \\(y = ax^b\\), then \\(log(y) = log(a) + b*log(x)\\) is linear. This makes the relationship easier to model and interpret."
  },
  {
    "objectID": "slides-03-02.html#transform-data-4",
    "href": "slides-03-02.html#transform-data-4",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\nWhen do we log-transform numeric variables?\n\nMaking multiplicative relationships additive: When variables have multiplicative effects (like compound growth), logging transforms them into additive relationships. This often makes more sense for how variables actually interact in real-world systems."
  },
  {
    "objectID": "slides-03-02.html#transform-data-5",
    "href": "slides-03-02.html#transform-data-5",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\nWhen do we log-transform numeric variables?\n\nStabilizing variance: When the variability of data increases with its magnitude (heteroscedasticity), log transformation can help create more constant variance across the range of values, which is important for many statistical methods."
  },
  {
    "objectID": "slides-03-03.html#data-modeling",
    "href": "slides-03-03.html#data-modeling",
    "title": "Intro to Data Modeling",
    "section": "Data Modeling",
    "text": "Data Modeling"
  },
  {
    "objectID": "slides-03-03.html#inference-vs.-prediction",
    "href": "slides-03-03.html#inference-vs.-prediction",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\nInference:\n\nunderstanding relationships and testing hypotheses about how variables interact\nexplain the underlying mechanisms and relationships in the data\nHow does education relates to income?"
  },
  {
    "objectID": "slides-03-03.html#inference-vs.-prediction-1",
    "href": "slides-03-03.html#inference-vs.-prediction-1",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\nPrediction:\n\ngenerating accurate estimates of future or unknown values\noptimizing the model’s ability to make correct predictions\nAccurately forecast someone’s income based on their education level."
  },
  {
    "objectID": "slides-03-03.html#inference-vs.-prediction-2",
    "href": "slides-03-03.html#inference-vs.-prediction-2",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\nInference models prioritize interpretability and often use simpler, more transparent methods like linear regression\nPrediction models may use more complex “black box” approaches like neural networks if they improve accuracy"
  },
  {
    "objectID": "slides-03-03.html#inference-vs.-prediction-3",
    "href": "slides-03-03.html#inference-vs.-prediction-3",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\nIn inference, you carefully choose variables based on theory and prior research\nFor prediction, you might include any feature that improves predictive performance, even if the relationship isn’t theoretically clear (! careful with this approach)"
  },
  {
    "objectID": "slides-03-03.html#inference-vs.-prediction-4",
    "href": "slides-03-03.html#inference-vs.-prediction-4",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\nInference focuses on metrics like p-values, confidence intervals, and effect sizes\nPrediction emphasizes metrics like mean squared error, classification accuracy, or the area under the ROC curve (AUC)"
  },
  {
    "objectID": "slides-03-03.html#inference-vs.-prediction-5",
    "href": "slides-03-03.html#inference-vs.-prediction-5",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\nOverfitting concerns\n\nboth approaches need to address overfitting\nit’s especially critical in prediction\ninference models might accept slightly worse fit for better interpretability\nprediction models focus heavily on cross-validation and out-of-sample performance"
  },
  {
    "objectID": "slides-03-03.html#inference",
    "href": "slides-03-03.html#inference",
    "title": "Intro to Data Modeling",
    "section": "Inference",
    "text": "Inference\nFor interpreting statistical models, we will be using the statsmodels Python package"
  },
  {
    "objectID": "slides-03-03.html#ordinary-least-squares-ols-regression",
    "href": "slides-03-03.html#ordinary-least-squares-ols-regression",
    "title": "Intro to Data Modeling",
    "section": "Ordinary Least Squares (OLS) regression",
    "text": "Ordinary Least Squares (OLS) regression\nOLS is a method that finds the best-fitting straight line through a set of points by minimizing the sum of squared vertical distances between the data points and the line.\nThe goal is to find a line y = β₀ + β₁x that best fits your data, where:\n\nβ₀ is the y-intercept\nβ₁ is the slope\nx is your independent variable – or feature(s)\ny is your dependent variable – or target"
  },
  {
    "objectID": "slides-03-03.html#ordinary-least-squares-ols-regression-1",
    "href": "slides-03-03.html#ordinary-least-squares-ols-regression-1",
    "title": "Intro to Data Modeling",
    "section": "Ordinary Least Squares (OLS) regression",
    "text": "Ordinary Least Squares (OLS) regression\nFor each data point, OLS:\n\nCalculates the vertical distance (residual) between the actual y-value and the predicted y-value\nSquares these distances (to make negatives positive and penalize larger errors more)\nSums all squared distances\nFinds the line that minimizes this sum"
  },
  {
    "objectID": "slides-03-03.html#ordinary-least-squares-ols-regression-2",
    "href": "slides-03-03.html#ordinary-least-squares-ols-regression-2",
    "title": "Intro to Data Modeling",
    "section": "Ordinary Least Squares (OLS) regression",
    "text": "Ordinary Least Squares (OLS) regression\nKey Assumptions:\n\nLinear relationship between variables\nIndependent observations\nHomoscedasticity (constant variance in errors)\nNormally distributed errors\nNo perfect multicollinearity"
  },
  {
    "objectID": "slides-03-03.html#ordinary-least-squares-ols-regression-3",
    "href": "slides-03-03.html#ordinary-least-squares-ols-regression-3",
    "title": "Intro to Data Modeling",
    "section": "Ordinary Least Squares (OLS) regression",
    "text": "Ordinary Least Squares (OLS) regression\nAdvantages:\n\nSimple to understand and implement\nBest Linear Unbiased Estimator (BLUE) under certain conditions\nComputationally efficient\nClear interpretation of results"
  },
  {
    "objectID": "slides-03-03.html#ordinary-least-squares-ols-regression-4",
    "href": "slides-03-03.html#ordinary-least-squares-ols-regression-4",
    "title": "Intro to Data Modeling",
    "section": "Ordinary Least Squares (OLS) regression",
    "text": "Ordinary Least Squares (OLS) regression\nLimitations:\n\nSensitive to outliers\nAssumes linear relationships\nMay not capture complex patterns\nAll predictors must be independent"
  },
  {
    "objectID": "slides-03-03.html#ols-regression-in-statsmodels",
    "href": "slides-03-03.html#ols-regression-in-statsmodels",
    "title": "Intro to Data Modeling",
    "section": "OLS regression in statsmodels",
    "text": "OLS regression in statsmodels\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n\ndef main():\n    data = pd.read_csv(\"data/clean_titanic.csv\")\n    X = data[[\"pclass\", \"sex\"]]  \n    y = data[\"survived\"] \n\n    # Ordinary Least Squares (regression)\n    result = sm.OLS(y, sm.add_constant(X)).fit()\n    print(result.summary())\n\nmain()"
  },
  {
    "objectID": "slides-03-03.html#ols-regression-results",
    "href": "slides-03-03.html#ols-regression-results",
    "title": "Intro to Data Modeling",
    "section": "OLS regression results",
    "text": "OLS regression results\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               survived   R-squared:                       0.366\nModel:                            OLS   Adj. R-squared:                  0.365\nMethod:                 Least Squares   F-statistic:                     255.2\nDate:                Tue, 11 Feb 2025   Prob (F-statistic):           3.19e-88\nTime:                        14:23:34   Log-Likelihood:                -417.77\nNo. Observations:                 887   AIC:                             841.5\nDf Residuals:                     884   BIC:                             855.9\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=============================================================================="
  },
  {
    "objectID": "slides-03-03.html#ols-regression-results-1",
    "href": "slides-03-03.html#ols-regression-results-1",
    "title": "Intro to Data Modeling",
    "section": "OLS regression results",
    "text": "OLS regression results\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.0825      0.040     26.795      0.000       1.003       1.162\npclass        -0.1577      0.016    -10.029      0.000      -0.189      -0.127\nsex           -0.5161      0.027    -18.776      0.000      -0.570      -0.462\n==============================================================================\nOmnibus:                       40.150   Durbin-Watson:                   1.921\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               44.755\nSkew:                           0.549   Prob(JB):                     1.91e-10\nKurtosis:                       3.060   Cond. No.                         9.07\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "slides-03-03.html#ols-regression-results-2",
    "href": "slides-03-03.html#ols-regression-results-2",
    "title": "Intro to Data Modeling",
    "section": "OLS regression results",
    "text": "OLS regression results\n\nWhat other variables can you add to the model?\nHow does the model change?"
  },
  {
    "objectID": "slides-03-03.html#ols-regression-results-3",
    "href": "slides-03-03.html#ols-regression-results-3",
    "title": "Intro to Data Modeling",
    "section": "OLS regression results",
    "text": "OLS regression results\nRun OLS regression on this kaggle dataset on house prices"
  },
  {
    "objectID": "modules.html#slides-3",
    "href": "modules.html#slides-3",
    "title": "Modules",
    "section": "Slides",
    "text": "Slides\n\nOLS case study\nLogistic Regression\nOLS vs. Logit – case study\nMachine Learning"
  },
  {
    "objectID": "slides-04-02.html#logistic-regression",
    "href": "slides-04-02.html#logistic-regression",
    "title": "Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nBinary Outcome – statistical model used for binary classification problems\nPredicts probability of an event occurring (between 0 and 1)\nUses the logistic/sigmoid function to transform predictions"
  },
  {
    "objectID": "slides-04-02.html#the-logistic-function",
    "href": "slides-04-02.html#the-logistic-function",
    "title": "Logistic Regression",
    "section": "The Logistic Function",
    "text": "The Logistic Function\n\nS-shaped curve (sigmoid)\nTransforms any input into a probability between 0 and 1\nFormula: \\(p = \\frac{1}{1 + e^{-z}}\\) where z is the linear predictor \\(\\beta_0 + \\beta_1 * x\\)"
  },
  {
    "objectID": "slides-04-02.html#assumptions",
    "href": "slides-04-02.html#assumptions",
    "title": "Logistic Regression",
    "section": "Assumptions",
    "text": "Assumptions\n\nIndependent observations\nNo multicollinearity among predictors\nLinear relationship between target (in log odds) and predictors\nAdequate sample size"
  },
  {
    "objectID": "slides-04-02.html#data",
    "href": "slides-04-02.html#data",
    "title": "Logistic Regression",
    "section": "Data",
    "text": "Data\nDownload Tagliamonte’s that expresion data and inspect it\nPhenomenom:\n\nI think that I will have a great time\nI think I will have a great time\n\nQuestion: What linguistics and social factor affect the expression of that"
  },
  {
    "objectID": "slides-04-02.html#logistic-regression-with-statsmodels",
    "href": "slides-04-02.html#logistic-regression-with-statsmodels",
    "title": "Logistic Regression",
    "section": "Logistic Regression with statsmodels",
    "text": "Logistic Regression with statsmodels\nDownload clean-that-expression.csv\n\nimport statsmodels.api as sm \nimport pandas as pd  \n\ndef main():\n    # loading the training dataset  \n    data = pd.read_csv(\"data/clean-that-expression.csv\") \n    \n    # defining the dependent and independent variables \n    X = data[[\"know\"]] \n    y = data[\"expressed\"]\n    \n    # building the model and fitting the data \n    log_reg = sm.Logit(y, sm.add_constant(X)).fit() \n    print(log_reg.summary()) \n\nmain()"
  },
  {
    "objectID": "slides-04-01.html#house-prices",
    "href": "slides-04-01.html#house-prices",
    "title": "OLS Case Study",
    "section": "House Prices",
    "text": "House Prices\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\ndef main():\n    data = pd.read_csv(\"data/clean_house_prices.csv\")\n\n    X = data[[\"bed\", \"house_size\"]]\n    y = data[\"price\"]\n\n    result = sm.OLS(y, sm.add_constant(X)).fit()\n    print(result.summary())\n\n\nmain()"
  },
  {
    "objectID": "slides-04-01.html#results-part-1",
    "href": "slides-04-01.html#results-part-1",
    "title": "OLS Case Study",
    "section": "Results – part 1",
    "text": "Results – part 1\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.242\nModel:                            OLS   Adj. R-squared:                  0.242\nMethod:                 Least Squares   F-statistic:                     1623.\nDate:                Sun, 16 Feb 2025   Prob (F-statistic):               0.00\nTime:                        09:06:45   Log-Likelihood:            -1.4998e+05\nNo. Observations:               10154   AIC:                         3.000e+05\nDf Residuals:                   10151   BIC:                         3.000e+05\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=============================================================================="
  },
  {
    "objectID": "slides-04-01.html#results-part-2",
    "href": "slides-04-01.html#results-part-2",
    "title": "OLS Case Study",
    "section": "Results – part 2",
    "text": "Results – part 2\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       -2.26e+04   1.92e+04     -1.177      0.239   -6.02e+04     1.5e+04\nbed        -2.675e+04   7072.066     -3.783      0.000   -4.06e+04   -1.29e+04\nhouse_size   391.1845      8.311     47.067      0.000     374.893     407.476\n==============================================================================\nOmnibus:                    12672.728   Durbin-Watson:                   1.202\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          3902146.529\nSkew:                           6.543   Prob(JB):                         0.00\nKurtosis:                      98.141   Cond. No.                     6.85e+03\n=============================================================================="
  },
  {
    "objectID": "slides-04-01.html#interpretation",
    "href": "slides-04-01.html#interpretation",
    "title": "OLS Case Study",
    "section": "Interpretation",
    "text": "Interpretation\nFor every added square foot, the price of the house goes up by 391.1845 dollars"
  },
  {
    "objectID": "slides-04-01.html#predicting-new-data",
    "href": "slides-04-01.html#predicting-new-data",
    "title": "OLS Case Study",
    "section": "Predicting new data",
    "text": "Predicting new data\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\ndef main():\n    data = pd.read_csv(\"data/clean_house_prices.csv\")\n\n    X = data[[\"bed\", \"house_size\", \"zip_code_99350\"]]\n    y = data[\"price\"]\n\n    result = sm.OLS(y, sm.add_constant(X)).fit()\n    print(result.summary())\n\n    data_dict = {\"bed\": [2, 3, 2], \n                 \"house_size\": [1400, 2300, 1400],\n                 \"zip_code_99350\": [0, 0, 1] }\n    df_dict = pd.DataFrame(data_dict)\n    new_data = sm.add_constant(df_dict[[\"bed\", \"house_size\", \"zip_code_99350\"]])\n    \n    predictions = result.predict(new_data)\n    print(predictions)\n\n\nmain()"
  },
  {
    "objectID": "slides-04-01.html#removing-columns-from-data",
    "href": "slides-04-01.html#removing-columns-from-data",
    "title": "OLS Case Study",
    "section": "Removing columns from data",
    "text": "Removing columns from data\n\ncolumns_to_exclude = [\"price\"]\nselected_data = data.drop(columns=columns_to_exclude)"
  },
  {
    "objectID": "slides-04-01.html#selecting-some-columns-from-data",
    "href": "slides-04-01.html#selecting-some-columns-from-data",
    "title": "OLS Case Study",
    "section": "Selecting some columns from data",
    "text": "Selecting some columns from data\n\nselected_data  = filtered_data.loc[:, [\"price\", \"bed\", \"house_size\", \"zip_code\"]]"
  },
  {
    "objectID": "slides-04-02.html#data-wrangling",
    "href": "slides-04-02.html#data-wrangling",
    "title": "Logistic Regression",
    "section": "Data wrangling",
    "text": "Data wrangling\n\nMake every variable in Tagliamonte’s that expresion data a numeric variable\nCall your python script data_wrangling.py\nYour script should read data/that-expression.csv and write data/clean-that-expression.csv\nSubmit it to gradescope\n\n\nassert isinstance(data.iloc[:,0][0], np.int64)\nassert isinstance(data.iloc[:,1][0], np.int64)\nassert isinstance(data.iloc[:,2][0], np.int64)"
  },
  {
    "objectID": "project-01.html",
    "href": "project-01.html",
    "title": "Project 1",
    "section": "",
    "text": "In this project, you will be fitting a linear regression model to tuition in the US data.\nDownload the tuition data and set up your coding environment."
  },
  {
    "objectID": "project-01.html#data-wrangling",
    "href": "project-01.html#data-wrangling",
    "title": "Project 1",
    "section": "Data wrangling",
    "text": "Data wrangling\nCreate a data_wrangling.py Python script and unsure that:\n\nthe year variable is an integer – remove the - and everything after the dash\nthe state variable is transformed into dummy variables, with one variable for each state, one hot encoded (with zeros and 1, integer format)\nyour main() function reads the original file in a folder called data (use pd.read_csv(\"data/tuition.csv\")) and write out a clean-tuition.csv file to the data folder (use .to_csv(\"data/clean-tuition.csv\"))\n\nHere’s what your clean-tuition.csv file should look like (showing first rows and first columns only):"
  },
  {
    "objectID": "project-01.html#plot",
    "href": "project-01.html#plot",
    "title": "Project 1",
    "section": "Plot",
    "text": "Plot\nCreate a plots.ipynb python notebook file and with the clean-tuition.csv data create a scatterplot of tuition vs. academic year.\nYou should replicate this scatterplot:"
  },
  {
    "objectID": "project-01.html#submit-to-gradescope",
    "href": "project-01.html#submit-to-gradescope",
    "title": "Project 1",
    "section": "Submit to gradescope",
    "text": "Submit to gradescope\nYou are to submit three files to gradescope:\n\ndata_wrangling.py\nplots.ipynb\nmodeling.py\n\nYou can submit other .py files if you are using any – don’t include any subfolders in your submission though, gradescope will not be considering any folders when running the code (only the data folder where .csv files should be placed).\nThe data gradescope will be using is slightly different from the data you are being provided with, but your code should be generalizable (meaning, no hardcoding of model coefficients, for example)."
  },
  {
    "objectID": "modules.html#assignment",
    "href": "modules.html#assignment",
    "title": "Modules",
    "section": "Assignment",
    "text": "Assignment\n\nProject 01 – due Friday, February 28, 11:59pm"
  },
  {
    "objectID": "project-01.html#data-modeling",
    "href": "project-01.html#data-modeling",
    "title": "Project 1",
    "section": "Data modeling",
    "text": "Data modeling\nCreate a modeling.py Python script and run Ordinary Least Squares on the clean-tution.csv data.\nEnsure that your main() function:\n\nprints out only the summary of the model, and the rsquared and rsquared adjusted values (in this order, your script should not print anything else) – you can use print(model.summary()), print(model.rsquared) and print(model.rsquared_adj) (replace model with the variable name you use in your script)\nwrites a coefficients.csv file with all of your model’s coefficients – you can use this block of code to based your code on (again, replace model with your variable name)\n\n\ncoeffs = model.params\ndf_coeffs = pd.DataFrame(coeffs)\ndf_coeffs.columns = [\"coefficient\"]\ndf_coeffs[\"variable\"] = df_coeffs.index\ndf_coeffs.to_csv(\"coefficients.csv\", index=False)\n\nHere’s what your coefficients.csv file should look like (showing first rows only):"
  },
  {
    "objectID": "slides-04-03.html#comparing-model-results",
    "href": "slides-04-03.html#comparing-model-results",
    "title": "OLS vs. Logit",
    "section": "Comparing model results",
    "text": "Comparing model results\nData: clean_titanic.csv\n\nCreate two Python scripts, one with OLS modeling, another with Logit modeling.\nWhat are the differences in the results?"
  },
  {
    "objectID": "slides-04-03.html#ols",
    "href": "slides-04-03.html#ols",
    "title": "OLS vs. Logit",
    "section": "OLS",
    "text": "OLS\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\ndef logit_to_probability(logit):\n  return 1 / (1 + np.exp(-logit))\n\ndef main():\n    data = pd.read_csv(\"data/clean_titanic.csv\")\n\n    X = data[[\"fare\", \"sex\", \"age\", \n          \"siblings_spouses_aboard\",\n          \"parents_children_aboard\"]]\n    y = data[\"survived\"]\n\n    model = sm.OLS(y, sm.add_constant(X)).fit()\n    print(model.summary())\n\n    coeffs = pd.DataFrame(model.params)\n    print(coeffs)\n    coeffs_prob = coeffs[0].apply(logit_to_probability)\n    print(coeffs_prob)\n\nmain()"
  },
  {
    "objectID": "slides-04-03.html#logit",
    "href": "slides-04-03.html#logit",
    "title": "OLS vs. Logit",
    "section": "Logit",
    "text": "Logit\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\ndef logit_to_probability(logit):\n  return 1 / (1 + np.exp(-logit))\n\ndef main():\n    data = pd.read_csv(\"data/clean_titanic.csv\")\n\n    X = data[[\"fare\", \"sex\", \"age\", \n          \"siblings_spouses_aboard\",\n          \"parents_children_aboard\"]]\n    y = data[\"survived\"]\n\n    model = sm.Logit(y, sm.add_constant(X)).fit()\n    print(model.summary())\n\n    coeffs = pd.DataFrame(model.params)\n    print(coeffs)\n    coeffs_prob = coeffs[0].apply(logit_to_probability)\n    print(coeffs_prob)\n\nmain()"
  },
  {
    "objectID": "slides-04-03.html#spam-vs.-ham",
    "href": "slides-04-03.html#spam-vs.-ham",
    "title": "OLS vs. Logit",
    "section": "Spam vs. Ham",
    "text": "Spam vs. Ham\nRun both OLS and Logit on this email data\n\nDownload the data\nClean the data\nWhat’s the target/response variable? What are the features/predictors?\nModel the data"
  },
  {
    "objectID": "slides-04-04.html#whats-machine-learning",
    "href": "slides-04-04.html#whats-machine-learning",
    "title": "Machine Learning (Prediction)",
    "section": "What’s machine learning?",
    "text": "What’s machine learning?\n\nmodeling to make predictions or find patterns in data\noften without explicit hypotheses\na flexible pattern-matcher looking for useful relationships in data\nmodels learn patterns and relationships within the data\nusually needs large amounts of data to train effective models"
  },
  {
    "objectID": "slides-04-04.html#scikit-learn",
    "href": "slides-04-04.html#scikit-learn",
    "title": "Machine Learning (Prediction)",
    "section": "Scikit-learn",
    "text": "Scikit-learn\n\noften shortened to sklearn\nopen-source machine learning library"
  },
  {
    "objectID": "slides-04-04.html#train-test",
    "href": "slides-04-04.html#train-test",
    "title": "Machine Learning (Prediction)",
    "section": "Train-Test",
    "text": "Train-Test\nSince the focus of machine learning is on model performance, we worry more about overfitting – we (validate and) test data that was not used in building the models (training)\nTesting (and validation) helps to assess how well the model generalizes to new, unseen data."
  },
  {
    "objectID": "slides-04-04.html#measuring-performance",
    "href": "slides-04-04.html#measuring-performance",
    "title": "Machine Learning (Prediction)",
    "section": "Measuring Performance",
    "text": "Measuring Performance\nRegression vs. Classification – different types of measures\nLet’s start with regression:\n\nMean Square Error (MSE):average of the squared differences between predicted and actual values\nSquare Root of MSE (RMSE): makes the error values more interpretable as they are in the same units as the target variable\nR-squared (R2 or \\(R^2\\)): the proportion of variance in the dependent variable explained by the model – ranges from 0 to 1, where 1 indicates a perfect fit."
  },
  {
    "objectID": "slides-04-04.html#train-test-with-sklearn",
    "href": "slides-04-04.html#train-test-with-sklearn",
    "title": "Machine Learning (Prediction)",
    "section": "Train-Test with sklearn",
    "text": "Train-Test with sklearn\nWe first load the method for splitting the data:\n\nfrom sklearn.model_selection import train_test_split\n\nThen, after reading the data in, we calltrain_test_split()\n\ntrain_test_split(X, y, test_size=0.3)\n\nDocumentation for train_test_split"
  },
  {
    "objectID": "slides-04-04.html#whats-machine-learning-1",
    "href": "slides-04-04.html#whats-machine-learning-1",
    "title": "Machine Learning (Prediction)",
    "section": "What’s machine learning?",
    "text": "What’s machine learning?\n\nless concerned with whether the data represents a random sample from a population\nfocus on predictive accuracy and generalization performance\ninterested in how well the model performs on new data"
  },
  {
    "objectID": "slides-04-04.html#scikit-learn-1",
    "href": "slides-04-04.html#scikit-learn-1",
    "title": "Machine Learning (Prediction)",
    "section": "Scikit-learn",
    "text": "Scikit-learn\n\nThis package includes:\n\nSupervised Learning Algorithms: Linear Regression, Random Forests, Support Vector Machines (SVM), and Neural Networks\nUnsupervised Learning Clustering Algorithms: K-means, PCA\nModel Selection Tools: cross-validation, parameter tuning, and metrics evaluation\nData Preprocessing Tools: scalers, encoders, and feature selection"
  },
  {
    "objectID": "slides-04-04.html#train-test-1",
    "href": "slides-04-04.html#train-test-1",
    "title": "Machine Learning (Prediction)",
    "section": "Train-Test",
    "text": "Train-Test\n\nTraining Set:\n\nused to train the machine learning model\nmost of the data (70% to 80%, usually)\n\nTesting Set:\n\nused to evaluate the model’s performance\nsmaller portion of the data that the model has not seen during training."
  },
  {
    "objectID": "slides-04-04.html#train-test-with-sklearn-1",
    "href": "slides-04-04.html#train-test-with-sklearn-1",
    "title": "Machine Learning (Prediction)",
    "section": "Train-Test with sklearn",
    "text": "Train-Test with sklearn\n\nrandom_state parameter: integer, seed value, to ensure reproducibility, sets the seed for the random number generator\n\nThe choice of random_state can impact the performance of your model, especially if the dataset is small or if the data points are not uniformly distributed.\nDifferent splits can lead to different training and testing sets, which in turn can affect the model’s performance metrics."
  },
  {
    "objectID": "slides-04-04.html#example",
    "href": "slides-04-04.html#example",
    "title": "Machine Learning (Prediction)",
    "section": "Example",
    "text": "Example\n\nimport random\nprint(random.randint(1, 10))\n\n7\n\n\n\nimport random\nrandom.seed(123)\nprint(random.randint(1, 10))\n\n1"
  },
  {
    "objectID": "slides-04-04.html#train-test-with-sklearn-2",
    "href": "slides-04-04.html#train-test-with-sklearn-2",
    "title": "Machine Learning (Prediction)",
    "section": "Train-Test with sklearn",
    "text": "Train-Test with sklearn\nLet’s use the clean email data for our first model (we will be running regression):\n\ndata = pd.read_csv(\"data/clean_email.csv\")\nX = data[[\"to_multiple\"]]\ny = data[\"spam\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)"
  },
  {
    "objectID": "slides-04-04.html#linear-regression",
    "href": "slides-04-04.html#linear-regression",
    "title": "Machine Learning (Prediction)",
    "section": "Linear Regression",
    "text": "Linear Regression\nWe will be using LinearRegression from sklearn.linear_model\n\nfrom sklearn.linear_model import LinearRegression\n\nThen we create and fit the model to our data\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"
  },
  {
    "objectID": "slides-04-04.html#assessing-model",
    "href": "slides-04-04.html#assessing-model",
    "title": "Machine Learning (Prediction)",
    "section": "Assessing model",
    "text": "Assessing model\nWe make predictions based on the test features\n\ny_pred = model.predict(X_test)\n\nThen we evaluate the model – lets calculate MSE and R2.\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nThen we call the appropiate methods on our observed and predicted targets:\n\nprint(mean_squared_error(y_test, y_pred))\nprint(r2_score(y_test, y_pred))"
  },
  {
    "objectID": "slides-04-04.html#scaling-features",
    "href": "slides-04-04.html#scaling-features",
    "title": "Machine Learning (Prediction)",
    "section": "Scaling features",
    "text": "Scaling features\nLet’s start with StandardScaler\n\nfrom sklearn.preprocessing import StandardScaler\n\nWe then scale our features for both training and testing data sets.\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)"
  },
  {
    "objectID": "slides-04-04.html#feature-scaling",
    "href": "slides-04-04.html#feature-scaling",
    "title": "Machine Learning (Prediction)",
    "section": "Feature scaling",
    "text": "Feature scaling\n\nPuts all features on a comparable level, so the model can properly assess their relative importance\nEnsures fair comparison between features – especially when features have very different scales (like age [0-100] vs. income [$0-$1,000,000]) (the model may incorrectly prioritize features with larger values)\nHelps avoid overflow or underflow issues in computations caused by very large or very small numbers can cause numerical"
  },
  {
    "objectID": "slides-04-04.html#feature-scaling-1",
    "href": "slides-04-04.html#feature-scaling-1",
    "title": "Machine Learning (Prediction)",
    "section": "Feature scaling",
    "text": "Feature scaling\nCommon scaling methods include:\n\nMin-Max Scaling: Scales features to a fixed range, usually [0,1]\nStandard Scaling: Transforms features to have zero mean and unit variance\nRobust Scaling: Uses statistics that are robust to outliers (like median and quartiles)"
  },
  {
    "objectID": "slides-05-01.html#train-test-with-sklearn",
    "href": "slides-05-01.html#train-test-with-sklearn",
    "title": "Introduction to Scikit-learn",
    "section": "Train-Test with sklearn",
    "text": "Train-Test with sklearn\nWe first load the method for splitting the data:\n\nfrom sklearn.model_selection import train_test_split\n\nThen, after reading the data in, we calltrain_test_split()\n\ntrain_test_split(X, y, test_size=0.3)\n\nDocumentation for train_test_split"
  },
  {
    "objectID": "slides-05-01.html#train-test-with-sklearn-1",
    "href": "slides-05-01.html#train-test-with-sklearn-1",
    "title": "Introduction to Scikit-learn",
    "section": "Train-Test with sklearn",
    "text": "Train-Test with sklearn\n\nrandom_state parameter: integer, seed value, to ensure reproducibility, sets the seed for the random number generator\n\nThe choice of random_state can impact the performance of your model, especially if the dataset is small or if the data points are not uniformly distributed.\nDifferent splits can lead to different training and testing sets, which in turn can affect the model’s performance metrics."
  },
  {
    "objectID": "slides-05-01.html#example",
    "href": "slides-05-01.html#example",
    "title": "Introduction to Scikit-learn",
    "section": "Example",
    "text": "Example\n\nimport random\nprint(random.randint(1, 10))\n\n8\n\n\n\nimport random\nrandom.seed(123)\nprint(random.randint(1, 10))\n\n1"
  },
  {
    "objectID": "slides-05-01.html#train-test-with-sklearn-2",
    "href": "slides-05-01.html#train-test-with-sklearn-2",
    "title": "Introduction to Scikit-learn",
    "section": "Train-Test with sklearn",
    "text": "Train-Test with sklearn\nLet’s use the clean email data for our first model (we will be running regression):\n\ndata = pd.read_csv(\"data/clean_email.csv\")\nX = data[[\"to_multiple\"]]\ny = data[\"spam\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)"
  },
  {
    "objectID": "slides-05-01.html#linear-regression",
    "href": "slides-05-01.html#linear-regression",
    "title": "Introduction to Scikit-learn",
    "section": "Linear Regression",
    "text": "Linear Regression\nWe will be using LinearRegression from sklearn.linear_model\n\nfrom sklearn.linear_model import LinearRegression\n\nThen we create and fit the model to our data\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"
  },
  {
    "objectID": "slides-05-01.html#assessing-model",
    "href": "slides-05-01.html#assessing-model",
    "title": "Introduction to Scikit-learn",
    "section": "Assessing model",
    "text": "Assessing model\nWe make predictions based on the test features\n\ny_pred = model.predict(X_test)\n\nThen we evaluate the model – lets calculate MSE and R2.\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nThen we call the appropiate methods on our observed and predicted targets:\n\nprint(mean_squared_error(y_test, y_pred))\nprint(r2_score(y_test, y_pred))"
  },
  {
    "objectID": "slides-05-01.html#feature-scaling",
    "href": "slides-05-01.html#feature-scaling",
    "title": "Introduction to Scikit-learn",
    "section": "Feature scaling",
    "text": "Feature scaling\n\nPuts all features on a comparable level, so the model can properly assess their relative importance\nEnsures fair comparison between features – especially when features have very different scales (like age [0-100] vs. income [$0-$1,000,000]) (the model may incorrectly prioritize features with larger values)\nHelps avoid overflow or underflow issues in computations caused by very large or very small numbers can cause numerical"
  },
  {
    "objectID": "slides-05-01.html#feature-scaling-1",
    "href": "slides-05-01.html#feature-scaling-1",
    "title": "Introduction to Scikit-learn",
    "section": "Feature scaling",
    "text": "Feature scaling\nCommon scaling methods include:\n\nMin-Max Scaling: Scales features to a fixed range, usually [0,1]\nStandard Scaling: Transforms features to have zero mean and unit variance\nRobust Scaling: Uses statistics that are robust to outliers (like median and quartiles)"
  },
  {
    "objectID": "slides-05-01.html#scaling-features",
    "href": "slides-05-01.html#scaling-features",
    "title": "Introduction to Scikit-learn",
    "section": "Scaling features",
    "text": "Scaling features\nLet’s start with StandardScaler\n\nfrom sklearn.preprocessing import StandardScaler\n\nWe then scale our features for both training and testing data sets.\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)"
  },
  {
    "objectID": "modules.html#slides-4",
    "href": "modules.html#slides-4",
    "title": "Modules",
    "section": "Slides",
    "text": "Slides\n\nIntroduction to scikit-learn"
  }
]